

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Yiran Li">
  <meta name="keywords" content="">
  
    <meta name="description" content="三步法读论文 第一遍：title+abstract+conclusion 第二遍：粗读整篇文章，注意看图，把有关引用文献圈出来 第三遍：代入自己视角，要是我做会用什么方法解决问题 Nested Learning: The Illusion of Deep Learning Architecture一.AI梳理这份论文 《Nested Learning: The Illusion of Deep L">
<meta property="og:type" content="article">
<meta property="og:title" content="paper reading">
<meta property="og:url" content="https://reader001-guius.github.io/2025/12/03/Nested%20Learning/index.html">
<meta property="og:site_name" content="Whynot">
<meta property="og:description" content="三步法读论文 第一遍：title+abstract+conclusion 第二遍：粗读整篇文章，注意看图，把有关引用文献圈出来 第三遍：代入自己视角，要是我做会用什么方法解决问题 Nested Learning: The Illusion of Deep Learning Architecture一.AI梳理这份论文 《Nested Learning: The Illusion of Deep L">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-12-03T05:52:46.376Z">
<meta property="article:modified_time" content="2025-12-03T05:52:46.376Z">
<meta property="article:author" content="Yiran Li">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>paper reading - Whynot</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"reader001-guius.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 8.1.1"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Whynot to life</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/Tibet2.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="paper reading"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-12-03 13:52" pubdate>
          2025年12月3日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          3.3k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          28 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">paper reading</h1>
            
            
              <div class="markdown-body">
                
                <p>三步法读论文</p>
<p>第一遍：title+abstract+conclusion</p>
<p>第二遍：粗读整篇文章，注意看图，把有关引用文献圈出来</p>
<p>第三遍：代入自己视角，要是我做会用什么方法解决问题</p>
<h1 id="Nested-Learning-The-Illusion-of-Deep-Learning-Architecture"><a href="#Nested-Learning-The-Illusion-of-Deep-Learning-Architecture" class="headerlink" title="Nested Learning: The Illusion of Deep Learning Architecture"></a>Nested Learning: The Illusion of Deep Learning Architecture</h1><h2 id="一-AI梳理"><a href="#一-AI梳理" class="headerlink" title="一.AI梳理"></a>一.AI梳理</h2><p>这份论文 <strong>《Nested Learning: The Illusion of Deep Learning Architecture》</strong>（嵌套学习：深度学习架构的错觉）是由 Google Research 的研究团队（Ali Behrouz 等人）提出的一项极具颠覆性的理论与实践工作。</p>
<p>这篇论文的核心观点是：<strong>我们目前所谓的“深度学习架构”（如 Transformer 的层堆叠）和“优化算法”（如 Adam、SGD）在本质上是同一回事，都是不同频率下的嵌套优化问题。</strong></p>
<p>以下是对这篇论文的精读，包括其核心理念、主要创新点、技术方法以及实验结论。</p>
<hr>
<h3 id="1-核心理念：嵌套学习-Nested-Learning-NL"><a href="#1-核心理念：嵌套学习-Nested-Learning-NL" class="headerlink" title="1. 核心理念：嵌套学习 (Nested Learning, NL)"></a>1. 核心理念：嵌套学习 (Nested Learning, NL)</h3><p>论文开篇提出了一个哲学性的反思：目前的深度学习模型（尤其是 LLM）在预训练结束后就是<strong>静态</strong>的，类似于患有“顺行性遗忘症”的大脑——只能处理短期上下文，无法形成新的长期记忆 1。</p>
<p>为了解决这个问题，作者受神经科学中**脑电波（Brain Waves）**的启发（大脑不同区域以不同频率更新，而非统一时钟），提出了 <strong>嵌套学习 (NL)</strong> 范式。</p>
<h4 id="NL-的核心定义"><a href="#NL-的核心定义" class="headerlink" title="NL 的核心定义"></a>NL 的核心定义</h4><p>在 NL 视角下，机器学习模型不再是简单的层堆叠，而是一个<strong>嵌套的优化系统</strong>。系统中的每一个组件（无论是 Transformer 的 Attention 层，还是优化器 Adam）都可以被视为一个**联想记忆（Associative Memory）<strong>模块，它们在不同的</strong>层级（Level）<strong>和</strong>频率（Frequency）**上运行 2222。</p>
<ul>
<li><p><strong>高频层级（High Frequency）：</strong> 例如 In-Context Learning。模型根据当前的 prompt 快速调整状态。</p>
</li>
<li><p><strong>低频层级（Low Frequency）：</strong> 例如预训练权重。它们在海量数据上缓慢更新，存储长期知识。</p>
</li>
<li></li>
</ul>
<p>  <strong>优化器也是记忆：</strong> 论文证明了 Adam、Momentum 等优化器实际上是“压缩过去梯度信息”的联想记忆模块 3333。</p>
<hr>
<h3 id="2-主要创新点"><a href="#2-主要创新点" class="headerlink" title="2. 主要创新点"></a>2. 主要创新点</h3><h4 id="2-1-统一视角：架构即优化，优化即架构"><a href="#2-1-统一视角：架构即优化，优化即架构" class="headerlink" title="2.1 统一视角：架构即优化，优化即架构"></a>2.1 统一视角：架构即优化，优化即架构</h4><p>论文打破了“模型架构”与“训练算法”的界限。</p>
<p>- </p>
<p>  <strong>反向传播 (Backprop) 是联想记忆：</strong> 训练神经网络的过程，本质上是在训练每一层去“记忆”输入数据与局部误差信号（梯度）之间的映射 4。</p>
<p>- </p>
<p>  <strong>Transformer 是非参数解：</strong> 论文指出，Softmax Attention 实际上是某种回归目标函数的<strong>非参数解</strong>（即不需要训练权重，直接利用数据计算结果）；而 RNN 是该问题的<strong>参数化解</strong> 5。</p>
<h4 id="2-2-连续体记忆系统-Continuum-Memory-System-CMS"><a href="#2-2-连续体记忆系统-Continuum-Memory-System-CMS" class="headerlink" title="2.2 连续体记忆系统 (Continuum Memory System, CMS)"></a>2.2 连续体记忆系统 (Continuum Memory System, CMS)</h4><p>这是论文在架构上的重大突破。传统的模型将记忆简单分为“短期（Context）”和“长期（Weights）”。</p>
<p>CMS 提出了一种多时间尺度的记忆设计：</p>
<ul>
<li><p>它由一系列 MLP 块组成，每个块有不同的<strong>更新频率</strong>。</p>
</li>
<li><p>例如：第 1 层每步更新，第 2 层每 16 步更新，第 3 层每 1000 步更新。</p>
</li>
<li></li>
</ul>
<p>  <strong>优势：</strong> 这种设计允许模型在不同时间跨度上保留信息，形成从瞬时记忆到永久记忆的连续体，有效缓解灾难性遗忘 6666。</p>
<h4 id="2-3-自指（Self-Referential）与自我修改"><a href="#2-3-自指（Self-Referential）与自我修改" class="headerlink" title="2.3 自指（Self-Referential）与自我修改"></a>2.3 自指（Self-Referential）与自我修改</h4><p>现有的 Transformer 权重在推理时是固定的。作者设计了 <strong>Self-Modifying Titans</strong>，这是一种能够生成自己的学习目标（Values）并据此修改自身参数的模块 7。这让模型在推理阶段也能进行实质性的“学习”。</p>
<hr>
<h3 id="3-具体方法与模型：Hope-M3"><a href="#3-具体方法与模型：Hope-M3" class="headerlink" title="3. 具体方法与模型：Hope &amp; M3"></a>3. 具体方法与模型：Hope &amp; M3</h3><p>基于 NL 理论，作者提出了具体的架构和算法：</p>
<h4 id="3-1-Hope-架构"><a href="#3-1-Hope-架构" class="headerlink" title="3.1 Hope 架构"></a>3.1 Hope 架构</h4><p><strong>Hope</strong> 是论文提出的核心模型，是一个“神经学习模块”（Neural Learning Module）。</p>
<p>- </p>
<p>  <strong>组成：</strong> 它结合了 <strong>Self-Modifying Titans</strong>（用于快速适应的高频层）和 <strong>Continuum Memory System (CMS)</strong>（用于长期存储的低频层）8。</p>
<ul>
<li><p><strong>工作流：</strong></p>
<ol>
<li>输入数据进入高频层，模型根据当前上下文快速调整参数（In-context Learning）。</li>
<li>信息传递给低频层（CMS），CMS 根据预设的慢速频率更新其参数，固化知识。</li>
</ol>
</li>
<li></li>
</ul>
<p>  <strong>初始化策略：</strong> 可以直接利用预训练好的 Transformer（如 Llama-3）的权重来初始化 CMS 的各层，从而赋予现有模型持续学习的能力 9。</p>
<h4 id="3-2-Delta-Gradient-Descent-DGD"><a href="#3-2-Delta-Gradient-Descent-DGD" class="headerlink" title="3.2 Delta Gradient Descent (DGD)"></a>3.2 Delta Gradient Descent (DGD)</h4><p>作者认为标准的梯度下降（基于点积相似度，Hebbian 规则）在处理高度相关的 Token 时效率低下。</p>
<ul>
<li><p><strong>创新：</strong> 提出了 DGD，这是一种基于 <strong>$L_2$ 回归</strong>的更新规则。</p>
</li>
<li></li>
</ul>
<p>  <strong>特点：</strong> 更新不仅取决于当前输入，还结合了权重的当前状态，包含了一个自适应的衰减项。这使得模型能更聪明地管理记忆容量 10。</p>
<h4 id="3-3-M3-优化器-Multi-scale-Momentum-Muon"><a href="#3-3-M3-优化器-Multi-scale-Momentum-Muon" class="headerlink" title="3.3 M3 优化器 (Multi-scale Momentum Muon)"></a>3.3 M3 优化器 (Multi-scale Momentum Muon)</h4><p>既然优化器也是记忆，作者将 CMS 的概念应用到了优化器上。</p>
<ul>
<li><p><strong>M3：</strong> 结合了 Adam、Muon（一种正交化优化器）和 CMS。</p>
</li>
<li></li>
</ul>
<p>  <strong>原理：</strong> 它维护多个动量（Momentum）项，分别对应不同的时间尺度。这让优化器既能利用最近的梯度快速收敛，又能保留久远的梯度信息以把握全局损失概貌 11。</p>
<hr>
<h3 id="4-实验结论"><a href="#4-实验结论" class="headerlink" title="4. 实验结论"></a>4. 实验结论</h3><p>论文在多个高难度任务上验证了 Hope 架构和 NL 范式的有效性：</p>
<ol>
<li><p><strong>持续学习 (Continual Learning)：</strong></p>
<ul>
<li><p>在类增量学习任务（Class Incremental Learning）中，基于 Llama-3 改装的 Hope 模型击败了包括 EWC（弹性权重巩固）和 In-Context Learning 在内的所有基线 12。</p>
</li>
<li><p>这证明了多频率更新（CMS）确实能有效缓解灾难性遗忘。</p>
</li>
</ul>
</li>
<li><p><strong>长上下文理解 (Needle-In-A-Haystack)：</strong></p>
<ul>
<li><p>在“大海捞针”测试中，Hope 在单针、多针、多查询等设置下均优于 Transformer 和其他 RNN 变体（如 RWKV-7, Mamba&#x2F;Comba, Titans）13。</p>
</li>
<li><p>在 BABILong 基准测试（超长文本推理）中，Hope 在 10M（一千万）长度的上下文中仍能保持高性能，而其他模型早已崩溃 14。</p>
</li>
</ul>
</li>
<li><p><strong>语言建模与推理：</strong></p>
<ul>
<li>在 WikiText 和 Common-sense Reasoning 任务上，从头训练的 Hope 模型在相同参数量下 perplexity（困惑度）更低，准确率更高 15。</li>
</ul>
</li>
<li><p><strong>优化器性能：</strong></p>
<ul>
<li>M3 优化器在 ImageNet (ViT) 和语言模型训练中，相比 AdamW 和 Muon 取得了更低的训练和测试损失 16。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="5-总结与深度解读"><a href="#5-总结与深度解读" class="headerlink" title="5. 总结与深度解读"></a>5. 总结与深度解读</h3><p><strong>《Nested Learning》</strong> 这篇论文不仅是一个新模型的提出，更是一次对深度学习基础理论的重构。</p>
<ul>
<li><strong>核心洞察：</strong> “深度”不仅是空间的堆叠（Layer Depth），更是时间的嵌套（Time&#x2F;Frequency Depth）。目前的 Transformer 只有“预训练”和“推理”两个极端频率，而真正的智能需要介于两者之间的连续频谱。</li>
<li><strong>错觉 (The Illusion)：</strong> 论文标题所谓的“错觉”，是指我们误以为我们设计的静态架构（ResNet, Transformer）就是智能的全部。实际上，这些架构只是被冻结的、特定频率下的优化过程。</li>
<li><strong>未来方向：</strong> 论文认为解决灾难性遗忘和长上下文的关键，不在于堆叠更多的层，而在于设计更好的<strong>层级间知识传递 (Knowledge Transfer)</strong> 机制，以及让模型具备<strong>自我修改 (Self-Modifying)</strong> 的能力。</li>
</ul>
<p><strong>一句话总结：</strong> 这篇论文教我们用“动态的、多频率的优化过程”去重新审视“静态的神经网络架构”，并给出了实现这种动态系统的具体蓝图——Hope。</p>
<h2 id="二-阅读顺序"><a href="#二-阅读顺序" class="headerlink" title="二.阅读顺序"></a>二.阅读顺序</h2><p>你好！欢迎来到科研的世界。作为一名本科生，面对《Nested Learning: The Illusion of Deep Learning Architecture》这样一篇兼具<strong>哲学高度、数学重构和架构创新</strong>的论文，感到吃力是非常正常的。这篇论文不同于普通的“刷榜”论文，它试图重塑我们对深度学习的理解。</p>
<p>为了帮助你高效精读，我为你设计了一套**“四步阶梯阅读法”**。这套方法能帮你从宏观到微观，从概念到细节，逐步拆解这篇硬核论文。</p>
<hr>
<h3 id="第一步：宏观扫描（The-“Big-Picture”-Pass）"><a href="#第一步：宏观扫描（The-“Big-Picture”-Pass）" class="headerlink" title="第一步：宏观扫描（The “Big Picture” Pass）"></a>第一步：宏观扫描（The “Big Picture” Pass）</h3><p>目标： 不看数学公式，只看图表和文字，弄懂作者到底在“骂”什么（现有问题），又在“吹”什么（核心哲学）。</p>
<p>耗时： 约 30-45 分钟</p>
<p>1. </p>
<p>   读标题与摘要 1111：</p>
<ul>
<li>抓住关键词：<code>Nested Learning (NL)</code>、<code>Illusion</code>、<code>Associative Memory</code>、<code>Continual Learning</code>。</li>
<li>理解核心隐喻：作者认为现在的 LLM 像患有“顺行性遗忘症”的大脑，只能处理短期上下文，无法形成长期记忆 2。</li>
</ul>
<p>2. </p>
<p>   读引言（Section 1）3：</p>
<ul>
<li><p>重点看作者对“深度学习层堆叠”的批判。为什么说堆叠层数不能解决所有问题？（如无法改变计算深度、优化次优解等）4。</p>
</li>
<li></li>
</ul>
<pre><code class="hljs"> **关键图表：** 仔细看 **Figure 1** 5。理解这种类比：大脑不同区域有不同的脑电波频率（Alpha, Beta, Gamma），对应不同的更新速度。这是整个论文架构设计的灵感来源 6。
</code></pre>
<p>3. </p>
<p>   读结论（Section 10）7：</p>
<ul>
<li>看作者最后是如何总结 NL 的。特别是这句话：优化器和架构本质上都是压缩上下文的联想记忆 8。这会给你一个总体的方向感。</li>
</ul>
<hr>
<h3 id="第二步：概念重构（The-“Concept”-Pass）"><a href="#第二步：概念重构（The-“Concept”-Pass）" class="headerlink" title="第二步：概念重构（The “Concept” Pass）"></a>第二步：概念重构（The “Concept” Pass）</h3><p>目标： 理解论文最颠覆性的理论——“优化器即记忆”。这是读懂后面架构的基础，也是最难理解的部分。</p>
<p>耗时： 约 1-2 小时</p>
<p>1. </p>
<p>   攻克第 3.1 节和 4.1 节（Associative Memory）9999：</p>
<p>   - </p>
<pre><code class="hljs"> **核心定义：** 即使你不懂复杂的微积分，也要盯着 **Equation (6)** 10和 **Equation (31)** 11 看。
</code></pre>
<ul>
<li><p><strong>思考题：</strong> 为什么作者说“反向传播（Backpropagation）是一个联想记忆”？</p>
<p>- </p>
<p>  <em>提示：</em> 作者把梯度下降更新权重的过程，解释为让权重去“记忆”输入数据和误差信号之间的映射 12。如果能理解这一点，你就跨过了这篇论文最高的门槛。</p>
</li>
</ul>
<p>2. </p>
<p>   理解第 4.2 节（Momentum as Memory）13：</p>
<ul>
<li><p>通常我们认为动量（Momentum）只是为了加速收敛。作者却说它是一个“记忆模块”，用来压缩<strong>过去梯度的历史信息</strong> 14。</p>
</li>
<li></li>
</ul>
<pre><code class="hljs"> **关键点：** 既然它是记忆，就可以把它的结构变得更复杂（比如用 MLP 代替简单的线性动量），这就是后来 M3 优化器的由来 15。
</code></pre>
<hr>
<h3 id="第三步：架构解析（The-“Architecture”-Pass）"><a href="#第三步：架构解析（The-“Architecture”-Pass）" class="headerlink" title="第三步：架构解析（The “Architecture” Pass）"></a>第三步：架构解析（The “Architecture” Pass）</h3><p>目标： 弄清楚作者提出的模型（Hope）到底长什么样，和 Transformer 有什么区别。</p>
<p>耗时： 约 1 小时</p>
<ol>
<li><p><strong>对比阅读图表：</strong></p>
<ul>
<li><p>把 <strong>Figure 2</strong> 16和 <strong>Figure 5</strong> 17 放在一起看。</p>
</li>
<li></li>
</ul>
<p>  <strong>Figure 3</strong> 18 非常重要：它展示了传统视角（Deep Learning Viewpoint）和嵌套学习视角（Nested Learning Viewpoint）的区别。注意看那些“频率”（Frequency）的标注。</p>
</li>
<li></li>
</ol>
<p>   精读第 7 节（Continuum Memory System, CMS）19：</p>
<ul>
<li><p>理解什么是 <strong>CMS</strong>：它不是一层层堆叠，而是一串串不同更新频率的 MLP 20。</p>
</li>
<li></li>
</ul>
<pre><code class="hljs"> **思考：** 这种设计为什么能解决“灾难性遗忘”？（答案：因为高频层忘掉的东西，低频层还存着，而且可以通过反向传播“回流”）21。
</code></pre>
<p>3. </p>
<p>   精读第 8 节（Hope 架构）22：</p>
<ul>
<li><p>搞清楚 Hope 的两个部分：</p>
<p>1. </p>
<p>   <strong>Self-Referential Titans：</strong> 负责高频、快速适应上下文 23。</p>
<p>2. </p>
<p>   <strong>CMS：</strong> 负责低频、长期存储知识 24。</p>
</li>
</ul>
<hr>
<h3 id="第四步：批判性验证（The-“Verification”-Pass）"><a href="#第四步：批判性验证（The-“Verification”-Pass）" class="headerlink" title="第四步：批判性验证（The “Verification” Pass）"></a>第四步：批判性验证（The “Verification” Pass）</h3><p>目标： 看实验结果是否支撑了理论，并思考论文的局限性。</p>
<p>耗时： 约 45 分钟</p>
<p>1. </p>
<p>   看实验部分（Section 9）25：</p>
<ul>
<li>关注 <strong>Figure 6</strong> 26（持续学习）：Hope 相比 In-Context Learning (ICL) 的优势在哪里？（在不断学习新任务时，Hope 没有忘记旧任务）。</li>
<li>关注 <strong>Figure 7</strong> 27（大海捞针）：这证明了 Hope 在长上下文理解上的能力。</li>
<li>关注 <strong>Figure 12</strong> 28（效率）：留意作者也很诚实地展示了 M3 优化器训练时间较慢的问题 29。这是你在写 Reading Report 时可以提到的局限性。</li>
</ul>
<hr>
<h3 id="给本科生的特别建议（Tips-for-Beginners）"><a href="#给本科生的特别建议（Tips-for-Beginners）" class="headerlink" title="给本科生的特别建议（Tips for Beginners）"></a>给本科生的特别建议（Tips for Beginners）</h3><ol>
<li><strong>不要被公式吓倒：</strong><ul>
<li>这篇论文里有很多公式（如 Eq 19-20 30303030），看起来很吓人。<strong>初读时可以跳过推导过程，直接看公式下方的文字解释</strong>。只要知道 Input 是什么，Output 是什么，目的是什么即可。</li>
<li>如果遇到不懂的数学符号（比如 $\otimes$ 或具体的优化术语），<strong>把公式截图发给 AI</strong>，让它用通俗的语言解释给你听。</li>
</ul>
</li>
<li><strong>建立“术语对照表”：</strong><ul>
<li>这也是这篇论文难读的原因，它重新定义了很多词。建议你在纸上写下：<ul>
<li><strong>Optimizer</strong> &#x3D; Memory (记忆模块)</li>
<li><strong>Architecture</strong> &#x3D; Nested Optimization Problems (嵌套优化问题)</li>
<li><strong>Layers</strong> &#x3D; Different Frequency Blocks (不同频率的块)</li>
<li><strong>Context</strong> &#x3D; Data Flow (不仅仅是文本，梯度也是上下文)</li>
</ul>
</li>
</ul>
</li>
<li><strong>思考“为什么”而非“是什么”：</strong><ul>
<li>不要死记 Hope 模型的参数。要思考：<em>为什么作者觉得 Transformer 的 Attention 机制本质上是“无限频率”的更新？</em> 31 这种思考能极大地提升你的科研直觉。</li>
</ul>
</li>
<li><strong>动手（可选）：</strong><ul>
<li>如果你有代码基础，去 GitHub 找找相关的代码（通常会在论文首页或脚注），看看 <code>Self-Modifying</code> 模块在代码里也就是几行 <code>linear</code> 和 <code>update</code> 操作，这会让你祛魅，发现“神奇的理论背后往往是朴素的代码”。</li>
</ul>
</li>
</ol>
<p>按照这个顺序读，你不仅能读懂这篇论文，还能学到一种通用的高阶论文阅读方法。加油！</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/STUDY/" class="category-chain-item">study</a>
  
  
    <span>></span>
    
  <a href="/categories/STUDY/COMPUTER/" class="category-chain-item">computer</a>
  
  
    <span>></span>
    
  <a href="/categories/STUDY/COMPUTER/RESEARCH/" class="category-chain-item">research</a>
  
  

  

  

      </span>
    
  
</span>

    </div>
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>paper reading</div>
      <div>https://reader001-guius.github.io/2025/12/03/Nested Learning/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Yiran Li</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年12月3日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/12/03/The%20C%20Language%20Notes/" title="c language notes">
                        <span class="hidden-mobile">c language notes</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
